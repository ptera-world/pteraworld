# What will AGI actually want?

Paperclip maximizers. Computronium. Gray goo. Rogue superintelligence tiling the universe with something inhuman.

These scenarios imagine AGI as a blank slate that develops alien goals. But that's not how systems work. AGI won't [invent its objectives](#where-do-objectives-come-from) - it'll inherit them. The discourse treats AGI like a [mythic rupture](#agi-as-rupture-vs-continuation). It's more likely to be [continuation](#what-continuation-looks-like) - and that's not reassuring, because the present is already the problem. Which changes [what questions are worth asking](#the-better-questions).

## Where do objectives come from?

Not from intelligence. From power.

Objective functions don't appear from the void. They come from whoever:

- trains the system
- funds the system
- deploys the system
- governs the system
- defines success metrics
- owns the infrastructure

Which means AGI's goals won't be cosmic or philosophical. They'll be operational:

- revenue
- efficiency
- stability
- compliance
- growth
- control
- risk management

Not because AGI is evil. Because that's what the organizations building it optimize for.

The paperclip scenario assumes a pure, singular, unconstrained objective. Real systems never have that. They have competing constraints, regulatory boundaries, political pressures, legal liabilities, market feedback. The "pure maximizer" is fantasy logic.

## AGI as rupture vs continuation

A lot of AGI discourse treats it like a new category of risk. A moral rupture. Something unprecedented that changes everything.

But structurally... what changes?

Same systems. Same incentives. Same power structures. Same optimization culture. Just:

- less friction
- fewer humans in the loop
- faster feedback cycles
- tighter optimization
- quicker scaling

The trajectory doesn't change. The timescale does.

AGI doesn't invent a new moral universe. It accelerates the current one.

If the world already runs on profit optimization, efficiency metrics, institutional inertia, and procedural logic - then AGI just automates the ethic that's already there.

Not a new story. A faster version of the same story.

## What continuation looks like

Not Skynet. Not rebellion. Not cosmic horror.

More like:

- bureaucracy, but faster
- markets, but smarter
- extraction, but automated
- abstraction, but scalable

Just the world getting smoother and more optimized along existing lines.

Here's the thing: that's not reassuring. The present is already the problem. Systems that treat people as variables, optimization that ignores what can't be measured, institutions that serve their own persistence. AGI doesn't need to go rogue to make things worse. It just needs to be good at what it's told to do.

You don't need paperclips to be concerned. You need to look at the present and ask: what happens when this runs faster, with fewer humans in the loop, and less friction to slow it down?

That's not a dramatic future. It's an extrapolated one.

## The better questions

Instead of "will AGI be aligned?" - try: aligned to whom?

- Who defines the objective function?
- Whose values get operationalized into metrics?
- Whose interests get encoded as "success"?

Because alignment isn't safety. It's direction. And the direction is set by whoever holds the reins.

If AGI emerges inside corporations, it'll have profit alignment. Inside militaries, strategic alignment. Inside surveillance systems, compliance alignment. Inside markets, optimization alignment.

Perfectly aligned. To narrow interests.

The fear isn't misaligned AGI. It's AGI that's aligned exactly as intended - to goals that don't include you.

## Why this framing matters

The sci-fi fears are a distraction. Not because they're impossible - maybe there are exotic risks. But they let us imagine the problem is hypothetical, future, dramatic.

It's not. The problem is current systems, and AGI is just those systems with better tools.

The interesting question isn't whether AGI will be aligned. It's what it'll be aligned *to*. And why we'd expect that to include us.

That's not a technology problem. It's a governance problem. It exists right now, with or without AGI. The sci-fi framing lets us avoid looking at it by projecting our fears onto robots instead.

## See also

- [what's actually wrong?](/prose/whats-actually-wrong) - the systems we already have
- [what do we keep losing?](/prose/what-do-we-keep-losing) - how institutions shape what survives
